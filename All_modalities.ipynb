{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNaRuNuMYLWV"
      },
      "source": [
        "# Catboost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaoqcegJUOIG",
        "outputId": "1ddeedf6-45d0-4adf-879f-403855fbbdb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.5-cp310-cp310-manylinux2014_x86_64.whl (98.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2.5\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-pUEIhZjUOKL"
      },
      "outputs": [],
      "source": [
        "from catboost import CatBoostRegressor, Pool\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "df_all = pd.read_csv('combined_ema.csv')\n",
        "\n",
        "columns_to_drop = [\n",
        "    'stressor_partner', 'stressor_fam', 'stressor_breakdown',\n",
        "    'stressor_money', 'stressor_selfcare', 'stressor_health',\n",
        "    'stressor_otherhealth', 'stressor_household', 'stressor_child',\n",
        "    'stressor_discrimination', 'stressor_none', 'moststressful',\n",
        "    'moststressful_time', 'work_location', 'attend_fidam', 'attend_fidpm',\n",
        "    'attend_hasp', 'attend_pgy1did', 'attend_pgy2did', 'attend_pgy3did',\n",
        "    'attend_none', 'work_start', 'work_end', 'jobperformance_best',\n",
        "    'jobsatisfaction', \"jobperformance2\",\"date\",\"Sleep1BeginTimestamp\",\n",
        "    'Sleep1EndTimestamp', 'Sleep2BeginTimestamp', 'Sleep2EndTimestamp', 'Sleep3BeginTimestamp', 'Sleep3EndTimestamp'\n",
        "]\n",
        "\n",
        "df_all.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "df_all['jobperformance'] = df_all['jobperformance'].replace(' ', np.nan)\n",
        "\n",
        "df_all.dropna(subset=['jobperformance'], inplace=True)\n",
        "\n",
        "df_all['jobperformance'] = df_all['jobperformance'].astype(float)\n",
        "\n",
        "Y = df_all['jobperformance'].astype(float)  # Ensure it is numeric\n",
        "X = df_all.drop(['jobperformance'], axis=1)\n",
        "\n",
        "for col in X.select_dtypes(include=['object']).columns:\n",
        "    X[col] = X[col].astype('category')\n",
        "\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "for col in X_train.select_dtypes(include=['category']).columns:\n",
        "    X_train[col] = X_train[col].astype(str).fillna('missing')\n",
        "    X_val[col] = X_val[col].astype(str).fillna('missing')\n",
        "\n",
        "model = CatBoostRegressor(verbose=False)\n",
        "model.fit(\n",
        "    X_train, Y_train,\n",
        "    cat_features=[X_train.columns.get_loc(c) for c in X_train.select_dtypes(['object']).columns]  # Now it expects object type since all are converted to string\n",
        ")\n",
        "\n",
        "predictions = model.predict(X_val)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2SzsoGnVXAg",
        "outputId": "bc42a4de-9164-40d2-d632-c1a5052c0a2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Data - Mean Squared Error: 0.22\n",
            "Training Data - R^2 Score: 0.83\n",
            "Validation Data - Mean Squared Error: 0.73\n",
            "Validation Data - R^2 Score: 0.48\n",
            "Training Data - CCC: 0.90\n",
            "Validation Data - CCC: 0.67\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def concordance_correlation_coefficient(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute the Concordance Correlation Coefficient for two arrays.\n",
        "    \"\"\"\n",
        "    mean_true = np.mean(y_true)\n",
        "    mean_pred = np.mean(y_pred)\n",
        "\n",
        "    var_true = np.var(y_true)\n",
        "    var_pred = np.var(y_pred)\n",
        "    covariance = np.mean((y_pred - mean_pred) * (y_true - mean_true))\n",
        "\n",
        "    ccc = (2 * covariance) / (var_pred + var_true + (mean_pred - mean_true) ** 2)\n",
        "\n",
        "    return ccc\n",
        "\n",
        "\n",
        "train_predictions = model.predict(X_train)\n",
        "\n",
        "val_predictions = model.predict(X_val)\n",
        "\n",
        "train_mse = mean_squared_error(Y_train, train_predictions)\n",
        "train_r2 = r2_score(Y_train, train_predictions)\n",
        "print(\"Training Data - Mean Squared Error: {:.2f}\".format(train_mse))\n",
        "print(\"Training Data - R^2 Score: {:.2f}\".format(train_r2))\n",
        "\n",
        "val_mse = mean_squared_error(Y_val, val_predictions)\n",
        "val_r2 = r2_score(Y_val, val_predictions)\n",
        "print(\"Validation Data - Mean Squared Error: {:.2f}\".format(val_mse))\n",
        "print(\"Validation Data - R^2 Score: {:.2f}\".format(val_r2))\n",
        "\n",
        "\n",
        "\n",
        "train_ccc = concordance_correlation_coefficient(Y_train, train_predictions)\n",
        "val_ccc = concordance_correlation_coefficient(Y_val, val_predictions)\n",
        "\n",
        "print(\"Training Data - CCC: {:.2f}\".format(train_ccc))\n",
        "print(\"Validation Data - CCC: {:.2f}\".format(val_ccc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLoQxfByWiLo",
        "outputId": "3b368a7f-857c-475a-cd4e-d8ddc5365291"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['id', 'completed_ts', 'Cardio_caloriesOut', 'Cardio_max', 'Cardio_min',\n",
              "       'Cardio_minutes', 'Fat Burn_caloriesOut', 'Fat Burn_max',\n",
              "       'Fat Burn_min', 'Fat Burn_minutes',\n",
              "       ...\n",
              "       'audio_86', 'survey_type', 'delivered_ts', 'started_ts', 'activity',\n",
              "       'location', 'atypical', 'stress', 'sleepquant', 'sleepqual'],\n",
              "      dtype='object', length=137)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_val.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIdG-ch8YXx7"
      },
      "source": [
        "# XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9lECkt70ZmUG"
      },
      "outputs": [],
      "source": [
        "X_train_xg = X_train.copy()\n",
        "X_val_xg = X_val.copy()\n",
        "Y_train_xg = Y_train.copy()\n",
        "Y_val_xg = Y_val.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "I5r1Luq8bj-A"
      },
      "outputs": [],
      "source": [
        "X_train_xg = X_train_xg.astype('category')\n",
        "X_val_xg = X_val.astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Uupqz0ocZkiE"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_enc = LabelEncoder()\n",
        "for column in X_train_xg.columns:\n",
        "    if X_train_xg[column].dtype == 'object':\n",
        "        X_train_xg[column].fillna('missing', inplace=True)\n",
        "        X_val_xg[column].fillna('missing', inplace=True)\n",
        "\n",
        "        X_train_xg[column] = label_enc.fit_transform(X_train_xg[column])\n",
        "        X_val_xg[column] = label_enc.transform(X_val_xg[column])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iahR8V_VuTy",
        "outputId": "2b94e35a-a7c9-46ee-d956-281b1e77c0ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost Training Data - Mean Squared Error: 0.38\n",
            "XGBoost Training Data - R² Score: 0.70\n",
            "XGBoost Validation Data - Mean Squared Error: 1.12\n",
            "XGBoost Validation Data - R² Score: 0.22\n",
            "XGBoost Training Data - CCC: 0.79\n",
            "XGBoost Validation Data - CCC: 0.33\n"
          ]
        }
      ],
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "xgb_model = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.6, learning_rate = 0.1,\n",
        "                             max_depth = 7, alpha = 10, n_estimators = 500,\n",
        "                                 enable_categorical=True,)\n",
        "\n",
        "xgb_model.fit(X_train_xg, Y_train_xg)\n",
        "\n",
        "xgb_train_predictions = xgb_model.predict(X_train_xg)\n",
        "\n",
        "xgb_val_predictions = xgb_model.predict(X_val_xg)\n",
        "\n",
        "xgb_train_mse = mean_squared_error(Y_train_xg, xgb_train_predictions)\n",
        "xgb_train_r2 = r2_score(Y_train_xg, xgb_train_predictions)\n",
        "\n",
        "xgb_val_mse = mean_squared_error(Y_val_xg, xgb_val_predictions)\n",
        "xgb_val_r2 = r2_score(Y_val_xg, xgb_val_predictions)\n",
        "\n",
        "print(\"XGBoost Training Data - Mean Squared Error: {:.2f}\".format(xgb_train_mse))\n",
        "print(\"XGBoost Training Data - R² Score: {:.2f}\".format(xgb_train_r2))\n",
        "print(\"XGBoost Validation Data - Mean Squared Error: {:.2f}\".format(xgb_val_mse))\n",
        "print(\"XGBoost Validation Data - R² Score: {:.2f}\".format(xgb_val_r2))\n",
        "\n",
        "def concordance_correlation_coefficient(y_true, y_pred):\n",
        "    mean_true = np.mean(y_true)\n",
        "    mean_pred = np.mean(y_pred)\n",
        "    var_true = np.var(y_true)\n",
        "    var_pred = np.var(y_pred)\n",
        "    covariance = np.mean((y_pred - mean_pred) * (y_true - mean_true))\n",
        "    ccc = (2 * covariance) / (var_pred + var_true + (mean_pred - mean_true) ** 2)\n",
        "    return ccc\n",
        "\n",
        "xgb_train_ccc = concordance_correlation_coefficient(Y_train_xg, xgb_train_predictions)\n",
        "xgb_val_ccc = concordance_correlation_coefficient(Y_val_xg, xgb_val_predictions)\n",
        "\n",
        "print(\"XGBoost Training Data - CCC: {:.2f}\".format(xgb_train_ccc))\n",
        "print(\"XGBoost Validation Data - CCC: {:.2f}\".format(xgb_val_ccc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsTz13aGchzw"
      },
      "source": [
        "# LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "la5tIr2Bd0cY"
      },
      "outputs": [],
      "source": [
        "Xt_backup = X_train.copy()\n",
        "Xv_backup = X_val.copy()\n",
        "Yt_backup = Y_train.copy()\n",
        "Yv_backup = Y_val.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SsVnWRvecslG"
      },
      "outputs": [],
      "source": [
        "X_train_l = X_train.copy()\n",
        "X_val_l = X_val.copy()\n",
        "Y_train_l = Y_train.copy()\n",
        "Y_val_l = Y_val.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5-MzYBTgamvl"
      },
      "outputs": [],
      "source": [
        "def convert_to_datetime(df, column):\n",
        "    df[column] = pd.to_datetime(df[column], errors='coerce')\n",
        "\n",
        "    if df[column].dtype == '<M8[ns]':  # Check if column is datetime\n",
        "        df[column + '_hour'] = df[column].dt.hour\n",
        "        df[column + '_weekday'] = df[column].dt.weekday\n",
        "    else:\n",
        "        df[column + '_hour'] = pd.NA\n",
        "        df[column + '_weekday'] = pd.NA\n",
        "\n",
        "columns_to_convert = ['delivered_ts', 'started_ts', 'completed_ts']\n",
        "for col in columns_to_convert:\n",
        "    convert_to_datetime(X_train, col)\n",
        "    convert_to_datetime(X_val, col)\n",
        "\n",
        "X_train.drop(columns_to_convert, axis=1, inplace=True)\n",
        "X_val.drop(columns_to_convert, axis=1, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tovw4l_GeEzC"
      },
      "outputs": [],
      "source": [
        "categorical_features = []\n",
        "for column in X_train.columns:\n",
        "    if X_train[column].dtype == 'object':\n",
        "        X_train[column] = X_train[column].astype('category')\n",
        "        X_val[column] = X_val[column].astype('category')\n",
        "        categorical_features.append(column)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Zde2mqleJwt",
        "outputId": "9d7ff328-a39a-4973-82e5-901c779d5b49"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:1873: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(f'{key} keyword has been found in `params` and will be ignored.\\n'\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:1893: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f'{cat_alias} in param dict is overridden.')\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:1873: UserWarning: categorical_feature keyword has been found in `params` and will be ignored.\n",
            "Please use categorical_feature argument of the Dataset constructor to pass this parameter.\n",
            "  _log_warning(f'{key} keyword has been found in `params` and will be ignored.\\n'\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:1893: UserWarning: categorical_feature in param dict is overridden.\n",
            "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] categorical_feature is set=id,survey_type,activity,location,atypical,stress,sleepquant,sleepqual,delivered_ts_hour,delivered_ts_weekday,started_ts_hour,started_ts_weekday,completed_ts_hour,completed_ts_weekday, categorical_column=0,127,128,129,130,131,132,133,134,135,136,137,138,139 will be ignored. Current value: categorical_feature=id,survey_type,activity,location,atypical,stress,sleepquant,sleepqual,delivered_ts_hour,delivered_ts_weekday,started_ts_hour,started_ts_weekday,completed_ts_hour,completed_ts_\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000400 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 8891\n",
            "[LightGBM] [Info] Number of data points in the train set: 542, number of used features: 121\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Start training from score 2.966790\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        }
      ],
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Initialize LightGBM\n",
        "lgb_model = lgb.LGBMRegressor(\n",
        "    objective='regression',\n",
        "    num_leaves=31,\n",
        "    learning_rate=0.05,\n",
        "    n_estimators=100,\n",
        "    categorical_feature=categorical_features  # Ensure categorical features are specified\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "lgb_model.fit(\n",
        "    X_train, Y_train,\n",
        "    eval_set=[(X_val, Y_val)],\n",
        "    eval_metric='mse',\n",
        ")\n",
        "\n",
        "# Predictions\n",
        "lgb_train_predictions = lgb_model.predict(X_train)\n",
        "lgb_val_predictions = lgb_model.predict(X_val)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeZAoaKMckXC",
        "outputId": "a849491c-d50c-4396-9cb0-02b3724313eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LightGBM\n",
            "Training MSE: 0.219, R2: 0.83\n",
            "Validation MSE: 0.784, R2: 0.45\n",
            "Training CCC: 0.894\n",
            "Validation CCC: 0.633\n"
          ]
        }
      ],
      "source": [
        "# Metrics\n",
        "print(\"LightGBM\")\n",
        "train_mse = mean_squared_error(Y_train, lgb_train_predictions)\n",
        "train_r2 = r2_score(Y_train, lgb_train_predictions)\n",
        "val_mse = mean_squared_error(Y_val, lgb_val_predictions)\n",
        "val_r2 = r2_score(Y_val, lgb_val_predictions)\n",
        "\n",
        "print(\"Training MSE: {:.3f}, R2: {:.2f}\".format(train_mse, train_r2))\n",
        "print(\"Validation MSE: {:.3f}, R2: {:.2f}\".format(val_mse, val_r2))\n",
        "\n",
        "train_ccc = concordance_correlation_coefficient(Y_train, lgb_train_predictions)\n",
        "val_ccc = concordance_correlation_coefficient(Y_val_l, lgb_val_predictions)\n",
        "\n",
        "print(\"Training CCC: {:.3f}\".format(train_ccc))\n",
        "print(\"Validation CCC: {:.3f}\".format(val_ccc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J42c1Wc3fpBN"
      },
      "source": [
        "# Sk-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "C8qgUxRjf6RX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "numeric_transformer = SimpleImputer(strategy='median')\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "X_train_prepared = preprocessor.fit_transform(X_train)\n",
        "X_val_prepared = preprocessor.transform(X_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSYC19cDemkS",
        "outputId": "d8339f5d-66aa-468b-b0ca-919178ea24b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest - Training MSE: 0.126, R²: 0.90\n",
            "Random Forest - Validation MSE: 0.767, R²: 0.46\n",
            "Training CCC: 0.938\n",
            "Validation CCC: 0.585\n",
            "SVM - Training MSE: 1.285, R²: -0.00\n",
            "SVM - Validation MSE: 1.381, R²: 0.03\n",
            "Training CCC: 0.009\n",
            "Validation CCC: 0.038\n",
            "Linear Regression - Training MSE: 0.282, R²: 0.78\n",
            "Linear Regression - Validation MSE: 762325.861, R²: -535082.27\n",
            "Training CCC: 0.876\n",
            "Validation CCC: 0.000\n",
            "Lasso - Training MSE: 1.092, R²: 0.15\n",
            "Lasso - Validation MSE: 1.341, R²: 0.06\n",
            "Training CCC: 0.239\n",
            "Validation CCC: 0.190\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.031e+02, tolerance: 6.954e-02\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "\n",
        "# Random Forest\n",
        "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_prepared, Y_train)\n",
        "rf_train_preds = rf_model.predict(X_train_prepared)\n",
        "rf_val_preds = rf_model.predict(X_val_prepared)\n",
        "\n",
        "# Support Vector Machine\n",
        "svm_model = SVR()\n",
        "svm_model.fit(X_train_prepared, Y_train)\n",
        "svm_train_preds = svm_model.predict(X_train_prepared)\n",
        "svm_val_preds = svm_model.predict(X_val_prepared)\n",
        "\n",
        "# Linear Regression\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train_prepared, Y_train)\n",
        "lr_train_preds = lr_model.predict(X_train_prepared)\n",
        "lr_val_preds = lr_model.predict(X_val_prepared)\n",
        "\n",
        "# Lasso Regression\n",
        "lasso_model = Lasso(alpha=0.1)\n",
        "lasso_model.fit(X_train_prepared, Y_train)\n",
        "lasso_train_preds = lasso_model.predict(X_train_prepared)\n",
        "lasso_val_preds = lasso_model.predict(X_val_prepared)\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "def evaluate_model(name, train_actuals, train_preds, val_actuals, val_preds):\n",
        "    train_mse = mean_squared_error(train_actuals, train_preds)\n",
        "    train_r2 = r2_score(train_actuals, train_preds)\n",
        "    val_mse = mean_squared_error(val_actuals, val_preds)\n",
        "    val_r2 = r2_score(val_actuals, val_preds)\n",
        "    print(f\"{name} - Training MSE: {train_mse:.3f}, R²: {train_r2:.2f}\")\n",
        "    print(f\"{name} - Validation MSE: {val_mse:.3f}, R²: {val_r2:.2f}\")\n",
        "\n",
        "evaluate_model('Random Forest', Y_train, rf_train_preds, Y_val, rf_val_preds)\n",
        "train_ccc = concordance_correlation_coefficient(Y_train, rf_train_preds)\n",
        "val_ccc = concordance_correlation_coefficient(Y_val, rf_val_preds)\n",
        "\n",
        "print(\"Training CCC: {:.3f}\".format(train_ccc))\n",
        "print(\"Validation CCC: {:.3f}\".format(val_ccc))\n",
        "\n",
        "evaluate_model('SVM', Y_train, svm_train_preds, Y_val, svm_val_preds)\n",
        "train_ccc = concordance_correlation_coefficient(Y_train, svm_train_preds)\n",
        "val_ccc = concordance_correlation_coefficient(Y_val, svm_val_preds)\n",
        "\n",
        "print(\"Training CCC: {:.3f}\".format(train_ccc))\n",
        "print(\"Validation CCC: {:.3f}\".format(val_ccc))\n",
        "evaluate_model('Linear Regression', Y_train, lr_train_preds, Y_val, lr_val_preds)\n",
        "train_ccc = concordance_correlation_coefficient(Y_train, lr_train_preds)\n",
        "val_ccc = concordance_correlation_coefficient(Y_val, lr_val_preds)\n",
        "\n",
        "print(\"Training CCC: {:.3f}\".format(train_ccc))\n",
        "print(\"Validation CCC: {:.3f}\".format(val_ccc))\n",
        "evaluate_model('Lasso', Y_train, lasso_train_preds, Y_val, lasso_val_preds)\n",
        "train_ccc = concordance_correlation_coefficient(Y_train, lasso_train_preds)\n",
        "val_ccc = concordance_correlation_coefficient(Y_val, lasso_val_preds)\n",
        "\n",
        "print(\"Training CCC: {:.3f}\".format(train_ccc))\n",
        "print(\"Validation CCC: {:.3f}\".format(val_ccc))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqcc-kpFlCKP"
      },
      "source": [
        "# Torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJx-i8TBf1XV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        scaler = StandardScaler()\n",
        "        self.X = torch.tensor(scaler.fit_transform(X), dtype=torch.float32)\n",
        "        self.y = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = TabularDataset(X_train_prepared, Y_train)\n",
        "val_dataset = TabularDataset(X_val_prepared, Y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKjcrPFHlGMk"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FeedForwardNet(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super(FeedForwardNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_features, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = FeedForwardNet(X_train_prepared.shape[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wesxsJCalI7T",
        "outputId": "0da916be-2819-41b1-d6e9-e2b735adc5f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 7.4012, Validation Loss: 8.0982\n",
            "Epoch 2, Loss: 2.5971, Validation Loss: 4.2371\n",
            "Epoch 3, Loss: 1.5607, Validation Loss: 2.3721\n",
            "Epoch 4, Loss: 1.6898, Validation Loss: 1.6664\n",
            "Epoch 5, Loss: 0.4629, Validation Loss: 1.4562\n",
            "Epoch 6, Loss: 0.3970, Validation Loss: 1.4188\n",
            "Epoch 7, Loss: 0.4185, Validation Loss: 1.3557\n",
            "Epoch 8, Loss: 0.3510, Validation Loss: 1.3434\n",
            "Epoch 9, Loss: 0.4413, Validation Loss: 1.2889\n",
            "Epoch 10, Loss: 0.2391, Validation Loss: 1.2890\n",
            "Epoch 11, Loss: 0.4684, Validation Loss: 1.2581\n",
            "Epoch 12, Loss: 0.2840, Validation Loss: 1.3051\n",
            "Epoch 13, Loss: 0.1758, Validation Loss: 1.2673\n",
            "Epoch 14, Loss: 0.3084, Validation Loss: 1.2957\n",
            "Epoch 15, Loss: 0.1823, Validation Loss: 1.2743\n",
            "Epoch 16, Loss: 0.1814, Validation Loss: 1.2643\n",
            "Epoch 17, Loss: 0.2694, Validation Loss: 1.2476\n",
            "Epoch 18, Loss: 0.1311, Validation Loss: 1.2143\n",
            "Epoch 19, Loss: 0.0943, Validation Loss: 1.2775\n",
            "Epoch 20, Loss: 0.2977, Validation Loss: 1.2373\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for data, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for data, targets in val_loader:\n",
        "                outputs = model(data)\n",
        "                val_loss += criterion(outputs, targets).item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}, Validation Loss: {val_loss / len(val_loader):.4f}\")\n",
        "\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nfYGO_clMz5",
        "outputId": "b44e9dc1-d4b0-4d24-dd36-1a7e1bc33977"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Val MSE: 1.4212, Val R²: 0.0024, Val CCC: 0.4926; Train loss : 0.10040497034788132\n",
            "Epoch 2: Val MSE: 1.3746, Val R²: 0.0352, Val CCC: 0.4969; Train loss : 0.09729180485010147\n",
            "Epoch 3: Val MSE: 1.4550, Val R²: -0.0213, Val CCC: 0.4966; Train loss : 0.09762842953205109\n",
            "Epoch 4: Val MSE: 1.4259, Val R²: -0.0009, Val CCC: 0.4957; Train loss : 0.09813300520181656\n",
            "Epoch 5: Val MSE: 1.4047, Val R²: 0.0140, Val CCC: 0.5007; Train loss : 0.08900494128465652\n",
            "Epoch 6: Val MSE: 1.4448, Val R²: -0.0142, Val CCC: 0.4887; Train loss : 0.07794305682182312\n",
            "Epoch 7: Val MSE: 1.5161, Val R²: -0.0642, Val CCC: 0.4906; Train loss : 0.09237392246723175\n",
            "Epoch 8: Val MSE: 1.4308, Val R²: -0.0043, Val CCC: 0.4910; Train loss : 0.0813034400343895\n",
            "Epoch 9: Val MSE: 1.4379, Val R²: -0.0093, Val CCC: 0.4964; Train loss : 0.07742319256067276\n",
            "Epoch 10: Val MSE: 1.3928, Val R²: 0.0224, Val CCC: 0.4961; Train loss : 0.07485932856798172\n",
            "Epoch 11: Val MSE: 1.4894, Val R²: -0.0454, Val CCC: 0.4882; Train loss : 0.074268639087677\n",
            "Epoch 12: Val MSE: 1.3776, Val R²: 0.0331, Val CCC: 0.4918; Train loss : 0.07147854566574097\n",
            "Epoch 13: Val MSE: 1.4242, Val R²: 0.0003, Val CCC: 0.4987; Train loss : 0.07256095111370087\n",
            "Epoch 14: Val MSE: 1.4216, Val R²: 0.0022, Val CCC: 0.4973; Train loss : 0.07008715718984604\n",
            "Epoch 15: Val MSE: 1.4376, Val R²: -0.0091, Val CCC: 0.4888; Train loss : 0.06524373590946198\n",
            "Epoch 16: Val MSE: 1.4323, Val R²: -0.0053, Val CCC: 0.5012; Train loss : 0.061240434646606445\n",
            "Epoch 17: Val MSE: 1.4677, Val R²: -0.0302, Val CCC: 0.4865; Train loss : 0.058538880199193954\n",
            "Epoch 18: Val MSE: 1.4310, Val R²: -0.0044, Val CCC: 0.4927; Train loss : 0.05864805728197098\n",
            "Epoch 19: Val MSE: 1.4651, Val R²: -0.0284, Val CCC: 0.4886; Train loss : 0.05413634330034256\n",
            "Epoch 20: Val MSE: 1.4400, Val R²: -0.0108, Val CCC: 0.4941; Train loss : 0.05265794321894646\n",
            "Final Validation MSE: 1.4400, Final Validation R²: -0.0108, Final Validation CCC: 0.4941\n"
          ]
        }
      ],
      "source": [
        "def train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        loss_lst = []\n",
        "        for data, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_lst += [loss]\n",
        "        loss = sum(loss_lst) / len(loss_lst)\n",
        "\n",
        "        model.eval()\n",
        "        actuals = []\n",
        "        predictions = []\n",
        "        with torch.no_grad():\n",
        "            for data, targets in val_loader:\n",
        "                outputs = model(data)\n",
        "                actuals.extend(targets.view(-1).tolist())\n",
        "                predictions.extend(outputs.view(-1).tolist())\n",
        "\n",
        "        mse = mean_squared_error(actuals, predictions)\n",
        "        r2 = r2_score(actuals, predictions)\n",
        "        ccc = concordance_correlation_coefficient(np.array(actuals), np.array(predictions))\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Val MSE: {mse:.4f}, Val R²: {r2:.4f}, Val CCC: {ccc:.4f}; Train loss : {loss}\")\n",
        "\n",
        "train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, epochs=20)\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    actuals = []\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for data, targets in loader:\n",
        "            outputs = model(data)\n",
        "            actuals.extend(targets.view(-1).tolist())\n",
        "            predictions.extend(outputs.view(-1).tolist())\n",
        "\n",
        "    mse = mean_squared_error(actuals, predictions)\n",
        "    r2 = r2_score(actuals, predictions)\n",
        "    ccc = concordance_correlation_coefficient(np.array(actuals), np.array(predictions))\n",
        "    return mse, r2, ccc\n",
        "\n",
        "final_mse, final_r2, final_ccc = evaluate_model(model, val_loader)\n",
        "print(f\"Final Validation MSE: {final_mse:.4f}, Final Validation R²: {final_r2:.4f}, Final Validation CCC: {final_ccc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
